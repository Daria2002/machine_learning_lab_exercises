{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sveučilište u Zagrebu  \n",
    "Fakultet elektrotehnike i računarstva  \n",
    "  \n",
    "## Strojno učenje 2018/2019  \n",
    "http://www.fer.unizg.hr/predmet/su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "### Laboratorijska vježba 1: Regresija\n",
    "\n",
    "*Verzija: 1.1  \n",
    "Zadnji put ažurirano: 12. listopada 2018.*\n",
    "\n",
    "(c) 2015-2018 Jan Šnajder, Domagoj Alagić, Mladen Karan \n",
    "\n",
    "Objavljeno: **12. listopada 2018.**  \n",
    "Rok za predaju: **22. listopada 2018. u 07:00h**\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upute\n",
    "\n",
    "Prva laboratorijska vježba sastoji se od osam zadataka. U nastavku slijedite upute navedene u ćelijama s tekstom. Rješavanje vježbe svodi se na **dopunjavanje ove bilježnice**: umetanja ćelije ili više njih **ispod** teksta zadatka, pisanja odgovarajućeg kôda te evaluiranja ćelija. \n",
    "\n",
    "Osigurajte da u potpunosti **razumijete** kôd koji ste napisali. Kod predaje vježbe, morate biti u stanju na zahtjev asistenta (ili demonstratora) preinačiti i ponovno evaluirati Vaš kôd. Nadalje, morate razumjeti teorijske osnove onoga što radite, u okvirima onoga što smo obradili na predavanju. Ispod nekih zadataka možete naći i pitanja koja služe kao smjernice za bolje razumijevanje gradiva (**nemojte pisati** odgovore na pitanja u bilježnicu). Stoga se nemojte ograničiti samo na to da riješite zadatak, nego slobodno eksperimentirajte. To upravo i jest svrha ovih vježbi.\n",
    "\n",
    "Vježbe trebate raditi **samostalno**. Možete se konzultirati s drugima o načelnom načinu rješavanja, ali u konačnici morate sami odraditi vježbu. U protivnome vježba nema smisla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Učitaj osnovne biblioteke..\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jednostavna regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadan je skup primjera $\\mathcal{D}=\\{(x^{(i)},y^{(i)})\\}_{i=1}^4 = \\{(0,4),(1,1),(2,2),(4,5)\\}$. Primjere predstavite matrixom $\\mathbf{X}$ dimenzija $N\\times n$ (u ovom slučaju $4\\times 1$) i vektorom oznaka $\\textbf{y}$, dimenzija $N\\times 1$ (u ovom slučaju $4\\times 1$), na sljedeći način:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [4]]\n",
      "[4 1 2 5]\n",
      "w1 [[ 0.29545455]\n",
      " [-0.21022727]\n",
      " [ 0.02840909]]\n",
      "w2 [[ 0.27272727]\n",
      " [ 0.11363636]\n",
      " [ 0.04545455]]\n",
      "w3 [[ 0.43181818]\n",
      " [ 0.09659091]\n",
      " [-0.07386364]]\n",
      "h1 [[ 0.06818182]\n",
      " [-0.07954545]\n",
      " [ 0.38068182]\n",
      " [ 0.55113636]\n",
      " [ 0.67613636]\n",
      " [ 0.40340909]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0],[1],[2],[4]])\n",
    "y = np.array([4,1,2,5])\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "xtilda = [[1,-3,1],[1,-3,3],[1,1,2],[1,2,1],[1,1,-2],[1,2,3]]\n",
    "y1=[[1],[1],[0],[0],[0],[0]]\n",
    "y2=[[0],[0],[1],[1],[0],[0]]\n",
    "y3=[[0],[0],[0],[0],[1],[1]]\n",
    "w1=dot(pinv(xtilda), y1)\n",
    "w2=dot(pinv(xtilda), y2)\n",
    "w3=dot(pinv(xtilda), y3)\n",
    "print('w1', w1)\n",
    "print('w2', w2)\n",
    "print('w3', w3)\n",
    "print('h1', dot( xtilda,w3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Proučite funkciju [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) iz biblioteke `sklearn` i upotrijebite je za generiranje matrice dizajna $\\mathbf{\\Phi}$ koja ne koristi preslikavanje u prostor više dimenzije (samo će svakom primjeru biti dodane *dummy* jedinice; $m=n+1$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  2.]\n",
      " [ 1.  4.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree = 1)\n",
    "theta = poly.fit_transform(X)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upoznajte se s modulom [`linalg`](http://docs.scipy.org/doc/numpy/reference/routines.linalg.html). Izračunajte težine $\\mathbf{w}$ modela linearne regresije kao $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Zatim se uvjerite da isti rezultat možete dobiti izračunom pseudoinverza $\\mathbf{\\Phi}^+$ matrice dizajna, tj. $\\mathbf{w}=\\mathbf{\\Phi}^+\\mathbf{y}$, korištenjem funkcije [`pinv`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4,)\n",
      "[ 2.2         0.45714286]\n",
      "[ 2.2         0.45714286]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(theta))\n",
    "print(np.shape(y))\n",
    "w = dot(dot(inv(dot(transpose(theta),theta)),transpose(theta)),y)\n",
    "print(w)\n",
    "w_izracunat_pomocu_pseudoinverza = dot(pinv(theta), y)\n",
    "print(w_izracunat_pomocu_pseudoinverza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radi jasnoće, u nastavku je vektor $\\mathbf{x}$ s dodanom *dummy* jedinicom $x_0=1$ označen kao $\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite primjere iz $\\mathcal{D}$ i funkciju $h(\\tilde{\\mathbf{x}})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$. Izračunajte pogrešku učenja prema izrazu $E(h|\\mathcal{D})=\\frac{1}{2}\\sum_{i=1}^N(\\tilde{\\mathbf{y}}^{(i)} - h(\\tilde{\\mathbf{x}}))^2$. Možete koristiti funkciju srednje kvadratne pogreške [`mean_squared_error`]( http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) iz modula [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).\n",
    "\n",
    "**Q:** Gore definirana funkcija pogreške $E(h|\\mathcal{D})$ i funkcija srednje kvadratne pogreške nisu posve identične. U čemu je razlika? Koja je \"realnija\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "4.08571428571\n",
      "2.04285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['poly']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15e4f252e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "plt.plot(X, y, 'x')\n",
    "\n",
    "def h_x_tilda(w, x_tilda):\n",
    "    return w[0]+w[1]*x_tilda\n",
    "\n",
    "x_data = linspace(0,5,100)\n",
    "plt.plot(x_data, h_x_tilda(w, x_data))\n",
    "\n",
    "def L_f(y, x, w):\n",
    "    h_x = h_x_tilda(w, x)\n",
    "    return (y-h_x)**2\n",
    "\n",
    "lista = [L_f(ely, elx[0], w) for (ely, elx) in zip(y, X)]\n",
    "E_rucno = 0.5 * sum(lista)\n",
    "\n",
    "E_pomocuFunkcije = mean_squared_error(y, h_x_tilda(w,X))\n",
    "print(E_rucno)\n",
    "print(E_pomocuFunkcije)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uvjerite se da za primjere iz $\\mathcal{D}$ težine $\\mathbf{w}$ ne možemo naći rješavanjem sustava $\\mathbf{w}=\\mathbf{\\Phi}^{-1}\\mathbf{y}$, već da nam doista treba pseudoinverz.\n",
    "\n",
    "**Q:** Zašto je to slučaj? Bi li se problem mogao riješiti preslikavanjem primjera u višu dimenziju? Ako da, bi li to uvijek funkcioniralo, neovisno o skupu primjera $\\mathcal{D}$? Pokažite na primjeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Last 2 dimensions of the array must be square",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3536f895b094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_makearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[0m_assertRankAtLeast2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m     \u001b[0m_assertNdSquareness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_assertNdSquareness\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Last 2 dimensions of the array must be square'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_assertFinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Last 2 dimensions of the array must be square"
     ]
    }
   ],
   "source": [
    "print(np.shape(theta))\n",
    "inv(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). Uvjerite se da su težine koje izračunava ta funkcija (dostupne pomoću atributa `coef_` i `intercept_`) jednake onima koje ste izračunali gore. Izračunajte predikcije modela (metoda `predict`) i uvjerite se da je pogreška učenja identična onoj koju ste ranije izračunali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, y)\n",
    "w_pomocu_funk_linreg = [reg.intercept_, reg.coef_[0]]\n",
    "print(w_pomocu_funk_linreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Polinomijalna regresija i utjecaj šuma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Razmotrimo sada regresiju na većem broju primjera. Koristite funkciju `make_labels(X, f, noise=0)` koja uzima matricu neoznačenih primjera $\\mathbf{X}_{N\\times n}$ te generira vektor njihovih oznaka $\\mathbf{y}_{N\\times 1}$. Oznake se generiraju kao $y^{(i)} = f(x^{(i)})+\\mathcal{N}(0,\\sigma^2)$, gdje je $f:\\mathbb{R}^n\\to\\mathbb{R}$ stvarna funkcija koja je generirala podatke (koja nam je u stvarnosti nepoznata), a $\\sigma$ je standardna devijacija Gaussovog šuma, definirana parametrom `noise`. Za generiranje šuma koristi se funkcija [`numpy.random.normal`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html). \n",
    "\n",
    "Generirajte skup za učenje od $N=50$ primjera uniformno distribuiranih u intervalu $[-5,5]$ pomoću funkcije $f(x) = 5 + x -2 x^2 -5 x^3$ uz šum  $\\sigma=200$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "def make_labels(X, f, noise=0) :\n",
    "    return map(lambda x : f(x) + (normal(0,noise) if noise>0 else 0), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_instances(x1, x2, N) :\n",
    "    return sp.array([np.array([x]) for x in np.linspace(x1,x2,N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "X = make_instances(-5, 5, 50)\n",
    "def f_x(x):\n",
    "    return 5+x-2*x**2-5*x**3\n",
    "\n",
    "Y = list(make_labels(X, f_x, 200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prikažite taj skup funkcijom [`scatter`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenirajte model polinomijalne regresije stupnja $d=3$. Na istom grafikonu prikažite naučeni model $h(\\mathbf{x})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$ i primjere za učenje. Izračunajte pogrešku učenja modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(x, y, w):\n",
    "    lista = [L_f(ely, elx[0], w) for ely, elx in zip(y, x)]\n",
    "    return 0.5*sum(lista)\n",
    "    \n",
    "def L_f(y, x, w):\n",
    "    h_x = h_x_tilda(w, x)\n",
    "    return (y-h_x)**2\n",
    "\n",
    "def h_x_tilda(w, x):\n",
    "    return dot(transpose(w),x)\n",
    "\n",
    "x_data = linspace(-5,5, 100)\n",
    "poly3 = PolynomialFeatures(degree = 3, include_bias=False)\n",
    "poly3dummy = PolynomialFeatures(degree=3, include_bias=True)\n",
    "#fit_transform je funkcija preslikavanja\n",
    "xtilda = poly3dummy.fit_transform(x_data.reshape([-1,1]))\n",
    "theta = poly3.fit_transform(X)\n",
    "reg = LinearRegression().fit(theta, Y)\n",
    "\n",
    "w=[]\n",
    "w.extend(reg.intercept_)\n",
    "w.extend(reg.coef_[0])\n",
    "\n",
    "h_x = dot(xtilda, w)\n",
    "plt.scatter(X, Y, c= 'r')\n",
    "plt.plot(x_data, h_x)\n",
    "plt.show()\n",
    "print(E(X, Y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Odabir modela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Na skupu podataka iz zadatka 2 trenirajte pet modela linearne regresije $\\mathcal{H}_d$ različite složenosti, gdje je $d$ stupanj polinoma, $d\\in\\{1,3,5,10,20\\}$. Prikažite na istome grafikonu skup za učenje i funkcije $h_d(\\mathbf{x})$ za svih pet modela (preporučujemo koristiti `plot` unutar `for` petlje). Izračunajte pogrešku učenja svakog od modela.\n",
    "\n",
    "**Q:** Koji model ima najmanju pogrešku učenja i zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [1, 3, 5, 10, 20]\n",
    "x_data = linspace(-5,5,1000).reshape([-1,1])\n",
    "\n",
    "def izracunajW(x, y):\n",
    "    return dot(pinv(x), y)\n",
    "\n",
    "def h_x (w, x):\n",
    "    return dot(x, w)\n",
    "\n",
    "def pogUcenja(ocekivano, dobiveno):\n",
    "    return 0.5*(sum((ocekivano-dobiveno)**2))\n",
    "\n",
    "plt.scatter(X, Y)\n",
    "\n",
    "for el in d:\n",
    "    poly = PolynomialFeatures(degree=el)\n",
    "    theta = poly.fit_transform(X)\n",
    "    x_data_transform = poly.fit_transform(x_data)\n",
    "    #salje se theta jer se mora poklapat w0+w1*x1+w2*x2\n",
    "    w = izracunajW(theta,Y)\n",
    "    hx = h_x(w, x_data_transform)\n",
    "    plt.plot(x_data, hx, label='d='+str(el))\n",
    "    plt.legend(loc=\"best\")\n",
    "    print('Pogreska ucenja za d={0} je {1}'.format(el, pogUcenja(Y, h_x(w, theta))))\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Razdvojite skup primjera iz zadatka 2 pomoću funkcije [`cross_validation.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split) na skup za učenja i skup za ispitivanje u omjeru 1:1. Prikažite na jednom grafikonu pogrešku učenja i ispitnu pogrešku za modele polinomijalne regresije $\\mathcal{H}_d$, sa stupnjem polinoma $d$ u rasponu $d\\in [1,2,\\ldots,20]$. Radi preciznosti, funkcije $h(\\mathbf{x})$ iscrtajte na cijelom skupu primjera (ali pogrešku generalizacije računajte, naravno, samo na ispitnome skupu). Budući da kvadratna pogreška brzo raste za veće stupnjeve polinoma, umjesto da iscrtate izravno iznose pogrešaka, iscrtajte njihove logaritme.\n",
    "\n",
    "**NB:** Podjela na skupa za učenje i skup za ispitivanje mora za svih pet modela biti identična.\n",
    "\n",
    "**Q:** Je li rezultat u skladu s očekivanjima? Koji biste model odabrali i zašto?\n",
    "\n",
    "**Q:** Pokrenite iscrtavanje više puta. U čemu je problem? Bi li problem bio jednako izražen kad bismo imali više primjera? Zašto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)\n",
    "\n",
    "x_train_6 = x_train\n",
    "x_test_6 = x_test\n",
    "y_train_6 = y_train\n",
    "y_test_6 = y_test\n",
    "\n",
    "def E(h_x,y):\n",
    "    return (0.5*sum((h_x-y)**2))\n",
    "\n",
    "def hx(w,x):\n",
    "    return dot(x, w)\n",
    "\n",
    "def get_weights(theta, y):\n",
    "    return dot(pinv(theta), y)\n",
    "\n",
    "Etrain = []\n",
    "Etest = []\n",
    "for i in range(1,21):\n",
    "    poly = PolynomialFeatures(degree = i, include_bias=True)\n",
    "    # u theta su dummy jedinice\n",
    "    theta_train = poly.fit_transform(x_train)\n",
    "    theta_test = poly.fit_transform(x_test)\n",
    "    w = get_weights(theta_train, y_train)\n",
    "    # h_x = w0+w1*x1+w2*x2+..\n",
    "    h_x_train = hx(w, theta_train)\n",
    "    h_x_test = hx(w, theta_test)\n",
    "    y_train = reshape(y_train, (-1,1))\n",
    "    Etrain.append(E(h_x_train, y_train))\n",
    "    Etest.append(E(h_x_test, y_test))\n",
    "\n",
    "plt.plot(range(1,21), log(Etrain), label='E_train')\n",
    "plt.plot(range(1,21), log(Etest), label='E_test')   \n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "Xprobaj = make_instances(-5, 5, 1000)\n",
    "\n",
    "Yprobaj = list(make_labels(Xprobaj, f_x, 200))\n",
    "\n",
    "xprobaj_train, xprobaj_test, yprobaj_train, yprobaj_test = train_test_split(Xprobaj, Yprobaj, test_size=0.5)\n",
    "\n",
    "Etrainprobaj = []\n",
    "Etestprobaj = []\n",
    "for i in range(1,21):\n",
    "    polyprobaj = PolynomialFeatures(degree = i, include_bias=True)\n",
    "    # u theta su dummy jedinice\n",
    "    thetaprobaj_train = polyprobaj.fit_transform(xprobaj_train)\n",
    "    thetaprobaj_test = polyprobaj.fit_transform(xprobaj_test)\n",
    "    wprobaj = get_weights(thetaprobaj_train, yprobaj_train)\n",
    "    # h_x = w0+w1*x1+w2*x2+..\n",
    "    h_x_trainprobaj = hx(wprobaj, thetaprobaj_train)\n",
    "    h_x_testprobaj = hx(wprobaj, thetaprobaj_test)\n",
    "    yprbaj_train = reshape(yprobaj_train, (-1,1))\n",
    "    Etrainprobaj.append(E(h_x_trainprobaj, yprobaj_train))\n",
    "    Etestprobaj.append(E(h_x_testprobaj, yprobaj_test))\n",
    "\n",
    "plt.plot(range(1,21), log(Etrainprobaj), label='E_train')\n",
    "plt.plot(range(1,21), log(Etestprobaj), label='E_test')   \n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Točnost modela ovisi o (1) njegovoj složenosti (stupanj $d$ polinoma), (2) broju primjera $N$, i (3) količini šuma. Kako biste to analizirali, nacrtajte grafikone pogrešaka kao u 3b, ali za sve kombinacija broja primjera $N\\in\\{100,200,1000\\}$ i količine šuma $\\sigma\\in\\{100,200,500\\}$ (ukupno 9 grafikona). Upotrijebite funkciju [`subplots`](http://matplotlib.org/examples/pylab_examples/subplots_demo.html) kako biste pregledno posložili grafikone u tablicu $3\\times 3$. Podatci se generiraju na isti način kao u zadatku 2.\n",
    "\n",
    "**NB:** Pobrinite se da svi grafikoni budu generirani nad usporedivim skupovima podataka, na sljedeći način. Generirajte najprije svih 1000 primjera, podijelite ih na skupove za učenje i skupove za ispitivanje (dva skupa od po 500 primjera). Zatim i od skupa za učenje i od skupa za ispitivanje načinite tri različite verzije, svaka s drugačijom količinom šuma (ukupno 2x3=6 verzija podataka). Kako bi simulirali veličinu skupa podataka, od tih dobivenih 6 skupova podataka uzorkujte trećinu, dvije trećine i sve podatke. Time ste dobili 18 skupova podataka -- skup za učenje i za testiranje za svaki od devet grafova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q:*** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "X = make_instances(-5, 5, 1000)\n",
    "x_train, x_test = train_test_split(X, test_size=0.5)\n",
    "\n",
    "x_train = list(x_train)\n",
    "x_test = list(x_test)\n",
    "\n",
    "x_train_N100 = np.array(random.sample(x_train,100))\n",
    "x_train_N200 = np.array(random.sample(x_train,200))\n",
    "x_train_N500 = np.array(random.sample(x_train,500))\n",
    "\n",
    "Ytrain_sum100_N100 = list(make_labels(x_train_N100, f_x, 100))\n",
    "Ytrain_sum100_N200 = list(make_labels(x_train_N200, f_x, 100))\n",
    "Ytrain_sum100_N500 = list(make_labels(x_train_N500, f_x, 100))\n",
    "\n",
    "Ytrain_sum200_N100 = list(make_labels(x_train_N100, f_x, 200))\n",
    "Ytrain_sum200_N200 = list(make_labels(x_train_N200, f_x, 200))\n",
    "Ytrain_sum200_N500 = list(make_labels(x_train_N500, f_x, 200))\n",
    "\n",
    "Ytrain_sum500_N100 = list(make_labels(x_train_N100, f_x, 500))\n",
    "Ytrain_sum500_N200 = list(make_labels(x_train_N200, f_x, 500))\n",
    "Ytrain_sum500_N500 = list(make_labels(x_train_N500, f_x, 500))\n",
    "\n",
    "x_test_N100 = np.array(random.sample(x_test,100))\n",
    "x_test_N200 = np.array(random.sample(x_test,200))\n",
    "x_test_N500 = np.array(random.sample(x_test,500))\n",
    "\n",
    "Ytest_sum100_N100 = list(make_labels(x_test_N100, f_x, 100))\n",
    "Ytest_sum100_N200 = list(make_labels(x_test_N200, f_x, 100))\n",
    "Ytest_sum100_N500 = list(make_labels(x_test_N500, f_x, 100))\n",
    "\n",
    "Ytest_sum200_N100 = list(make_labels(x_test_N100, f_x, 200))\n",
    "Ytest_sum200_N200 = list(make_labels(x_test_N200, f_x, 200))\n",
    "Ytest_sum200_N500 = list(make_labels(x_test_N500, f_x, 200))\n",
    "\n",
    "Ytest_sum500_N100 = list(make_labels(x_test_N100, f_x, 500))\n",
    "Ytest_sum500_N200 = list(make_labels(x_test_N200, f_x, 500))\n",
    "Ytest_sum500_N500 = list(make_labels(x_test_N500, f_x, 500))\n",
    "\n",
    "\n",
    "def get_weight(theta, y):\n",
    "    return dot(pinv(theta), y)\n",
    "\n",
    "def greska(ocekivano, stvarno):\n",
    "    return (0.5*sum((ocekivano-stvarno)**2))\n",
    "\n",
    "def ocekivanje(w, x):\n",
    "    return dot(x, w)\n",
    "\n",
    "def prikaz(brGrafa, x, y):\n",
    "    subplot(3,3,brGrafa)\n",
    "    Etrain=[]\n",
    "    vrati = []\n",
    "    for i in range(1, 21):\n",
    "        poly = PolynomialFeatures(degree=i, include_bias=True)\n",
    "        theta = poly.fit_transform(x)\n",
    "        w=get_weight(theta, y)\n",
    "        vrati.append(w)\n",
    "        h_x = ocekivanje(w, theta)\n",
    "        Etrain.append(greska(y, h_x))\n",
    "    plt.plot(range(1,21), log(Etrain), c='g')\n",
    "    return vrati\n",
    "\n",
    "def prikazTest(brGrafa, x, y, w):\n",
    "    subplot(3,3,brGrafa)\n",
    "    Etest=[]\n",
    "    for i in range(1, 21):\n",
    "        poly = PolynomialFeatures(degree=i, include_bias=True)\n",
    "        theta = poly.fit_transform(x)\n",
    "        h_x = ocekivanje(w[i-1], theta)\n",
    "        Etest.append(greska(y, h_x))\n",
    "    plt.plot(range(1,21), log(Etest), c='r')\n",
    "    \n",
    "w1=prikaz(1, x_train_N100, Ytrain_sum100_N100)\n",
    "w2=prikaz(2, x_train_N200, Ytrain_sum100_N200)\n",
    "w3=prikaz(3, x_train_N500, Ytrain_sum100_N500)\n",
    "w4=prikaz(4, x_train_N100, Ytrain_sum200_N100)\n",
    "w5=prikaz(5, x_train_N200, Ytrain_sum200_N200)\n",
    "w6=prikaz(6, x_train_N500, Ytrain_sum200_N500)\n",
    "w7=prikaz(7, x_train_N100, Ytrain_sum500_N100)\n",
    "w8=prikaz(8, x_train_N200, Ytrain_sum500_N200)\n",
    "w9=prikaz(9, x_train_N500, Ytrain_sum500_N500)\n",
    "  \n",
    "prikazTest(1, x_test_N100, Ytest_sum100_N100,w1)\n",
    "prikazTest(2, x_test_N200, Ytest_sum100_N200,w2)\n",
    "prikazTest(3, x_test_N500, Ytest_sum100_N500,w3)\n",
    "prikazTest(4, x_test_N100, Ytest_sum200_N100,w4)\n",
    "prikazTest(5, x_test_N200, Ytest_sum200_N200,w5)\n",
    "prikazTest(6, x_test_N500, Ytest_sum200_N500,w6)\n",
    "prikazTest(7, x_test_N100, Ytest_sum500_N100,w7)\n",
    "prikazTest(8, x_test_N200, Ytest_sum500_N200,w8)\n",
    "prikazTest(9, x_test_N500, Ytest_sum500_N500,w9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regularizirana regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "U gornjim eksperimentima nismo koristili **regularizaciju**. Vratimo se najprije na primjer iz zadatka 1. Na primjerima iz tog zadatka izračunajte težine $\\mathbf{w}$ za polinomijalni regresijski model stupnja $d=3$ uz L2-regularizaciju (tzv. *ridge regression*), prema izrazu $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Napravite izračun težina za regularizacijske faktore $\\lambda=0$, $\\lambda=1$ i $\\lambda=10$ te usporedite dobivene težine.\n",
    "\n",
    "**Q:** Kojih je dimenzija matrica koju treba invertirati?\n",
    "\n",
    "**Q:** Po čemu se razlikuju dobivene težine i je li ta razlika očekivana? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0],[1],[2],[4]])\n",
    "y = np.array([4,1,2,5])\n",
    "\n",
    "lambde = [0,1,10]\n",
    "poly3dummy = PolynomialFeatures(degree=3, include_bias=True)\n",
    "theta = poly3dummy.fit_transform(X)\n",
    "#m+1=3+1=d+1\n",
    "#w0 se ne regularizira\n",
    "\n",
    "def racunajW(lambda_, theta, y):\n",
    "    matrica = np.eye(4)\n",
    "    matrica[0][0]=0\n",
    "    return inv(transpose(theta).dot(theta)+la*matrica).dot(transpose(theta)).dot(y)\n",
    "\n",
    "for la in lambde:\n",
    "    w = racunajW(la, theta, y)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite klasu [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model), koja implementira L2-regularizirani regresijski model. Parametar $\\alpha$ odgovara parametru $\\lambda$. Primijenite model na istim primjerima kao u prethodnom zadatku i ispišite težine $\\mathbf{w}$ (atributi `coef_` i `intercept_`).\n",
    "\n",
    "**Q:** Jesu li težine identične onima iz zadatka 4a? Ako nisu, objasnite zašto je to tako i kako biste to popravili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for la in lambde:\n",
    "    w = []\n",
    "    ri = Ridge(alpha = la)\n",
    "    ri.fit(theta, y)\n",
    "    w.append(ri.intercept_)\n",
    "    for coef in ri.coef_[1:4]:\n",
    "        w.append(coef)\n",
    "    print('w:', w)\n",
    "    \n",
    "for la in lambde:\n",
    "    w = []\n",
    "    ri = Ridge(alpha = la)\n",
    "    ri.fit(theta, y)\n",
    "    w.append(round(ri.intercept_, 8))\n",
    "    for coef in ri.coef_[1:4]:\n",
    "        w.append(round(coef, 8))\n",
    "    print('w_zaokruzenp:', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regularizirana polinomijalna regresija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Vratimo se na slučaj $N=50$ slučajno generiranih primjera iz zadatka 2. Trenirajte modele polinomijalne regresije $\\mathcal{H}_{\\lambda,d}$ za $\\lambda\\in\\{0,100\\}$ i $d\\in\\{2,10\\}$ (ukupno četiri modela). Skicirajte pripadne funkcije $h(\\mathbf{x})$ i primjere (na jednom grafikonu; preporučujemo koristiti `plot` unutar `for` petlje).\n",
    "\n",
    "**Q:** Jesu li rezultati očekivani? Obrazložite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(X, f, noise=0) :\n",
    "    return map(lambda x : f(x) + (normal(0,noise) if noise>0 else 0), X)\n",
    "\n",
    "def make_instances(x1, x2, N) :\n",
    "    return sp.array([np.array([x]) for x in np.linspace(x1,x2,N)])\n",
    "\n",
    "def f_x(x):\n",
    "    return 5+x-2*x**2-5*x**3\n",
    "\n",
    "def racunajW(lambda_, theta, y, stupanj):\n",
    "    matrica = np.eye(stupanj+1)\n",
    "    matrica[0][0]=0\n",
    "    return inv(transpose(theta).dot(theta)+lambda_*matrica).dot(transpose(theta)).dot(y)\n",
    "\n",
    "def racunaj_hx(w, x):\n",
    "    return dot(x, w.reshape([-1,1]))\n",
    "\n",
    "def nacrtaj_hx(w, xtrans, x, stupanj, lambda_):\n",
    "    hx = racunaj_hx(w, xtrans)\n",
    "    hx = hx.reshape([1,-1])\n",
    "    plt.plot(x, hx[0], label='lambda={0}, a stupanj={1}'.format(lambda_, stupanj))\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "\n",
    "X = make_instances(-5, 5, 50)\n",
    "Y = list(make_labels(X, f_x, 200))\n",
    "plt.scatter(X,Y)\n",
    "\n",
    "lambde = [0, 100]\n",
    "d = [2,10]\n",
    "\n",
    "for lambda_ in lambde:\n",
    "    for stupanj in d:\n",
    "        ridge = Ridge(alpha=lambda_, fit_intercept=False)\n",
    "        poly = PolynomialFeatures(degree = stupanj)\n",
    "        theta = poly.fit_transform(X)\n",
    "        w = ridge.fit(theta, Y).coef_\n",
    "        x_data = linspace(-5,5,1000).reshape([-1,1])\n",
    "        x_data_transform = poly.fit_transform(x_data)\n",
    "        nacrtaj_hx(w, x_data_transform, x_data, stupanj, lambda_)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "Kao u zadataku 3b, razdvojite primjere na skup za učenje i skup za ispitivanje u omjeru 1:1. Prikažite krivulje logaritama pogreške učenja i ispitne pogreške u ovisnosti za model $\\mathcal{H}_{d=20,\\lambda}$, podešavajući faktor regularizacije $\\lambda$ u rasponu $\\lambda\\in\\{0,1,\\dots,50\\}$.\n",
    "\n",
    "**Q:** Kojoj strani na grafikonu odgovara područje prenaučenosti, a kojoj podnaučenosti? Zašto?\n",
    "\n",
    "**Q:** Koju biste vrijednosti za $\\lambda$ izabrali na temelju ovih grafikona i zašto?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = 20\n",
    "\n",
    "def izracunaj_E(ocekivano, dobiveno, lambda_, w):\n",
    "    help = 0.5*(sum((dobiveno-ocekivano)**2))\n",
    "    return (help+lambda_*0.5*(np.linalg.norm(w))**2)\n",
    "\n",
    "def izracunaj_hx(theta, w):\n",
    "    return dot(theta, w.reshape([-1,1]))\n",
    "\n",
    "EtrainLista=[]\n",
    "EtestLista = []\n",
    "\n",
    "for lambda_ in range(0, 51):\n",
    "    ridge = Ridge(alpha=lambda_)\n",
    "    poly = PolynomialFeatures(degree=d, include_bias=True)\n",
    "    x_train_trans = poly.fit_transform(x_train)\n",
    "    train = ridge.fit(x_train_trans, y_train)\n",
    "    w = train.coef_\n",
    "    hx = izracunaj_hx(x_train_trans, w)\n",
    "    y_dobiven = hx\n",
    "    E_train = izracunaj_E(y_train, y_dobiven, lambda_, w)\n",
    "    EtrainLista.append(E_train)\n",
    "    \n",
    "    x_test_trans = poly.fit_transform(x_test)\n",
    "    hx_test = izracunaj_hx(x_test_trans, w)\n",
    "    y_dobiven_test = hx_test\n",
    "    E_test = izracunaj_E(y_test, y_dobiven_test, lambda_, w)\n",
    "    EtestLista.append(E_test)\n",
    "\n",
    "plot(range(0, 51), log(EtrainLista), label='train')\n",
    "plot(range(0, 51), log(EtestLista), label='test')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('E')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. L1-regularizacija i L2-regularizacija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Svrha regularizacije jest potiskivanje težina modela $\\mathbf{w}$ prema nuli, kako bi model bio što jednostavniji. Složenost modela može se okarakterizirati normom pripadnog vektora težina $\\mathbf{w}$, i to tipično L2-normom ili L1-normom. Za jednom trenirani model možemo izračunati i broj ne-nul značajki, ili L0-normu, pomoću sljedeće funkcije:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonzeroes(coef, tol=1e-6): \n",
    "    return len(coef) - len(coef[sp.isclose(0, coef, atol=tol)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Za ovaj zadatak upotrijebite skup za učenje i skup za testiranje iz zadatka 3b. Trenirajte modele **L2-regularizirane** polinomijalne regresije stupnja $d=20$, mijenjajući hiperparametar $\\lambda$ u rasponu $\\{1,2,\\dots,100\\}$. Za svaki od treniranih modela izračunajte L{0,1,2}-norme vektora težina $\\mathbf{w}$ te ih prikažite kao funkciju od $\\lambda$.\n",
    "\n",
    "**Q:** Objasnite oblik obiju krivulja. Hoće li krivulja za $\\|\\mathbf{w}\\|_2$ doseći nulu? Zašto? Je li to problem? Zašto?\n",
    "\n",
    "**Q:** Za $\\lambda=100$, koliki je postotak težina modela jednak nuli, odnosno koliko je model rijedak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def racunaj_L0(w):\n",
    "    return nonzeroes(w)\n",
    "\n",
    "def racunaj_L1(w):\n",
    "    return sum(absolute(w_) for w_ in w)\n",
    "\n",
    "def racunaj_L2(w):\n",
    "    return sqrt(sum((w_**2) for w_ in w))\n",
    "\n",
    "d=20\n",
    "\n",
    "L0_list = []\n",
    "L1_list = []\n",
    "L2_list = []\n",
    "\n",
    "for lambda_ in range(1,101):\n",
    "    ridge = Ridge(alpha=lambda_, fit_intercept=False)\n",
    "    poly = PolynomialFeatures(degree=d)\n",
    "    x_train_6_trans = poly.fit_transform(x_train_6)\n",
    "    train = ridge.fit(x_train_6_trans, y_train)\n",
    "    w = train.coef_[0]\n",
    "    L0 = racunaj_L0(w)\n",
    "    L1 = racunaj_L1(w)\n",
    "    L2 = racunaj_L2(w)\n",
    "    L0_list.append(L0)\n",
    "    L1_list.append(L1)\n",
    "    L2_list.append(L2)\n",
    "    \n",
    "print('w=', w)\n",
    "print('postotak nenula:', nonzeroes(w)/w.size)\n",
    "plt.plot(range(1,101), L0_list, label='L0')\n",
    "plt.plot(range(1,101), L1_list, label='L1')\n",
    "plt.plot(range(1,101), L2_list, label='L2')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('norma od w')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glavna prednost L1-regularizirane regresije (ili *LASSO regression*) nad L2-regulariziranom regresijom jest u tome što L1-regularizirana regresija rezultira **rijetkim modelima** (engl. *sparse models*), odnosno modelima kod kojih su mnoge težine pritegnute na nulu. Pokažite da je to doista tako, ponovivši gornji eksperiment s **L1-regulariziranom** regresijom, implementiranom u klasi  [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) u modulu [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "L0_list = []\n",
    "L1_list = []\n",
    "L2_list = []\n",
    "\n",
    "for lambda_ in range(1,101):\n",
    "    clf = linear_model.Lasso(alpha=lambda_)\n",
    "    w = clf.fit(x_train_6_trans, y_train).coef_\n",
    "    L0 = racunaj_L0(w)\n",
    "    L1 = racunaj_L1(w)\n",
    "    L2 = racunaj_L2(w)\n",
    "    L0_list.append(L0)\n",
    "    L1_list.append(L1)\n",
    "    L2_list.append(L2)\n",
    "        \n",
    "plt.plot(range(1,101), L0_list, label='L0')\n",
    "plt.plot(range(1,101), L1_list, label='L1')\n",
    "plt.plot(range(1,101), L2_list, label='L2')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Značajke različitih skala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Često se u praksi možemo susreti sa podatcima u kojima sve značajke nisu jednakih magnituda. Primjer jednog takvog skupa je regresijski skup podataka `grades` u kojem se predviđa prosjek ocjena studenta na studiju (1--5) na temelju dvije značajke: bodova na prijamnom ispitu (1--3000) i prosjeka ocjena u srednjoj školi. Prosjek ocjena na studiju izračunat je kao težinska suma ove dvije značajke uz dodani šum.\n",
    "\n",
    "Koristite sljedeći kôd kako biste generirali ovaj skup podataka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_data_points = 500\n",
    "np.random.seed(69)\n",
    "\n",
    "# Generiraj podatke o bodovima na prijamnom ispitu koristeći normalnu razdiobu i ograniči ih na interval [1, 3000].\n",
    "exam_score = np.random.normal(loc=1500.0, scale = 500.0, size = n_data_points) \n",
    "exam_score = np.round(exam_score)\n",
    "exam_score[exam_score > 3000] = 3000\n",
    "exam_score[exam_score < 0] = 0\n",
    "\n",
    "# Generiraj podatke o ocjenama iz srednje škole koristeći normalnu razdiobu i ograniči ih na interval [1, 5].\n",
    "grade_in_highschool = np.random.normal(loc=3, scale = 2.0, size = n_data_points)\n",
    "grade_in_highschool[grade_in_highschool > 5] = 5\n",
    "grade_in_highschool[grade_in_highschool < 1] = 1\n",
    "\n",
    "# Matrica dizajna.\n",
    "grades_X = np.array([exam_score,grade_in_highschool]).T\n",
    "\n",
    "# Završno, generiraj izlazne vrijednosti.\n",
    "rand_noise = np.random.normal(loc=0.0, scale = 0.5, size = n_data_points)\n",
    "exam_influence = 0.9\n",
    "grades_y = ((exam_score / 3000.0) * (exam_influence) + (grade_in_highschool / 5.0) \\\n",
    "            * (1.0 - exam_influence)) * 5.0 + rand_noise\n",
    "grades_y[grades_y < 1] = 1\n",
    "grades_y[grades_y > 5] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iscrtajte ovisnost ciljne vrijednosti (y-os) o prvoj i o drugoj značajki (x-os). Iscrtajte dva odvojena grafa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(exam_score, grades_y)\n",
    "plt.xlabel('bodovi na prijemnom')\n",
    "plt.ylabel('prosjek na faksu')\n",
    "plt.show()\n",
    "plt.scatter(grade_in_highschool, grades_y, label='ocjene u sk')\n",
    "plt.xlabel('prosjek u srednjoj sk')\n",
    "plt.ylabel('prosjek na faksu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naučite model L2-regularizirane regresije ($\\lambda = 0.01$), na podacima `grades_X` i `grades_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.01\n",
    "ridge = Ridge(alpha=lam, fit_intercept=False)\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "grades_X_trans = poly.fit_transform(grades_X)\n",
    "train = ridge.fit(grades_X_trans, grades_y)\n",
    "w = train.coef_\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ponovite gornji eksperiment, ali prvo skalirajte podatke `grades_X` i `grades_y` i spremite ih u varijable `grades_X_fixed` i `grades_y_fixed`. Za tu svrhu, koristite [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerx = StandardScaler()\n",
    "scalerx.fit(grades_X)\n",
    "grades_x_fixed = scalerx.transform(grades_X)\n",
    "\n",
    "grades_y = grades_y.reshape([-1,1])\n",
    "scalery = StandardScaler()\n",
    "scalery.fit(grades_y)\n",
    "grades_y_fixed = scalery.transform(grades_y)\n",
    "\n",
    "lam = 0.01\n",
    "ridge = Ridge(alpha=lam, fit_intercept=False)\n",
    "grades_x_fixed_trans = poly.fit_transform(grades_x_fixed)\n",
    "train = ridge.fit(grades_x_fixed_trans, grades_y_fixed)\n",
    "w=train.coef_[0]\n",
    "print(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Gledajući grafikone iz podzadatka (a), koja značajka bi trebala imati veću magnitudu, odnosno važnost pri predikciji prosjeka na studiju? Odgovaraju li težine Vašoj intuiciji? Objasnite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multikolinearnost i kondicija matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izradite skup podataka `grades_X_fixed_colinear` tako što ćete u skupu `grades_X_fixed` iz\n",
    "zadatka 7b duplicirati zadnji stupac (ocjenu iz srednje škole). Time smo efektivno uveli savršenu multikolinearnost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zadnji_red = grades_x_fixed[:,1:].reshape([-1,1])\n",
    "grades_X_fixed_colinear= np.hstack([grades_x_fixed, zadnji_red])\n",
    "print(grades_X_fixed_colinear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponovno, naučite na ovom skupu L2-regularizirani model regresije ($\\lambda = 0.01$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.01\n",
    "ridge = Ridge(alpha=lam, fit_intercept=False)\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "grades_X_fixed_colinear_trans = poly.fit_transform(grades_X_fixed_colinear)\n",
    "w = ridge.fit(grades_X_fixed_colinear_trans, grades_y).coef_[0]\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Usporedite iznose težina s onima koje ste dobili u zadatku *7b*. Što se dogodilo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slučajno uzorkujte 50% elemenata iz skupa `grades_X_fixed_colinear` i naučite dva modela L2-regularizirane regresije, jedan s $\\lambda=0.01$, a jedan s $\\lambda=1000$. Ponovite ovaj pokus 10 puta (svaki put s drugim podskupom od 50% elemenata).  Za svaki model, ispišite dobiveni vektor težina u svih 10 ponavljanja te ispišite standardnu devijaciju vrijednosti svake od težina (ukupno šest standardnih devijacija, svaka dobivena nad 10 vrijednosti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def izracunajW(x, y, lam):\n",
    "    poly = PolynomialFeatures(degree=1, include_bias=False)\n",
    "    x_trans = poly.fit_transform(x)\n",
    "    ridge = Ridge(alpha=lam, fit_intercept=False)\n",
    "    w = ridge.fit(x_trans, y).coef_[0]\n",
    "    return w\n",
    "\n",
    "w0_001 = []\n",
    "w0_1000 = []\n",
    "w1_001 = []\n",
    "w1_1000 = []\n",
    "w2_001 = []\n",
    "w2_1000 = []\n",
    "\n",
    "pom=[]\n",
    "for i in range(0, 10):\n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(grades_X_fixed_colinear, grades_y, test_size = 0.5)\n",
    "    \n",
    "    w0_001.append((izracunajW(x_train, y_train, 0.01))[0])\n",
    "    w0_1000.append((izracunajW(x_train, y_train, 1000))[0])\n",
    "    \n",
    "    w1_001.append((izracunajW(x_train, y_train, 0.01))[1])\n",
    "    w1_1000.append((izracunajW(x_train, y_train, 1000))[1])\n",
    "    \n",
    "    w2_001.append((izracunajW(x_train, y_train, 0.01))[2])\n",
    "    w2_1000.append((izracunajW(x_train, y_train, 1000))[2])\n",
    "     \n",
    "    print('w_001',izracunajW(x_train, y_train, 0.01))\n",
    "    print('w_1000',izracunajW(x_train, y_train, 1000))\n",
    "    \n",
    "stdev_w0_001 = np.std(w0_001)\n",
    "print('stand dev za w0, lambda=0.01:', stdev_w0_001)\n",
    "stdev_w0_1000 = np.std(w0_1000)\n",
    "print('stand dev za w0, lambda=1000:', stdev_w0_1000)\n",
    "\n",
    "stdev_w1_001 = np.std(w1_001)\n",
    "print('stand dev za w1, lambda=0.01:', stdev_w1_001)\n",
    "stdev_w1_1000 = np.std(w1_1000)\n",
    "print('stand dev za w1, lambda=1000:', stdev_w1_1000)\n",
    "\n",
    "stdev_w2_001 = np.std(w2_001)\n",
    "print('stand dev za w2, lambda=0.01:', stdev_w2_001)\n",
    "stdev_w2_1000 = np.std(w2_1000)\n",
    "print('stand dev za w2, lambda=1000:', stdev_w2_1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako regularizacija utječe na stabilnost težina?  \n",
    "**Q:** Jesu li koeficijenti jednakih magnituda kao u prethodnom pokusu? Objasnite zašto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koristeći [`numpy.linalg.cond`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.cond.html) izračunajte kondicijski broj matrice $\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I}$, gdje je $\\mathbf{\\Phi}$ matrica dizajna (`grades_fixed_X_colinear`). Ponovite i za $\\lambda=0.01$ i za $\\lambda=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kondicijskiBroj(x, lambda_):\n",
    "    matricaEye = eye(np.shape(x)[1])\n",
    "    matricaEye[0][0]=0\n",
    "    poly = PolynomialFeatures(degree=1, include_bias=False)\n",
    "    theta = poly.fit_transform(x)\n",
    "    first = dot(transpose(theta), theta)\n",
    "    return np.linalg.cond(dot(transpose(theta), theta)+ lambda_*matricaEye)\n",
    "\n",
    "lambde = [0.01, 10]\n",
    "\n",
    "for i in lambde:\n",
    "    print(kondicijskiBroj(grades_X_fixed_colinear, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako regularizacija utječe na kondicijski broj matrice $\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I}$?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
